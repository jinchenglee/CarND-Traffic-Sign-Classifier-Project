{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load TDB Notebook extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.utils.load_extensions('tdb_ext/main')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.utils.load_extensions('tdb_ext/main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tdb\n",
    "from tdb.examples import mnist, viz\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TSC net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### 3. TSC_Net class\n",
    "# \n",
    "# The Traffic Sign Classifier (TSC) Neural Network implemention.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "LOG_DIR = './tb_log/2xLeNet_256_8e-4_dropout'\n",
    "MODEL_DIR =  './model/2xLeNet_256_8e-4_dropout'\n",
    "\n",
    "TRAIN_DROPOUT = 0.5 \n",
    "TEST_DROPOUT = 1.0\n",
    "\n",
    "\n",
    "class tsc_net():\n",
    "    \"\"\"\n",
    "    Traffic Sign Classification Network class. Derived from LeNet example.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.create_tsc_net()\n",
    "        self.create_tf()\n",
    "        \n",
    "        self.session = tf.InteractiveSession()\n",
    "        \n",
    "        # Merge all summaries and write them out\n",
    "        self.merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "        # Summary saving directories\n",
    "        if not os.path.exists(LOG_DIR):\n",
    "            os.makedirs(LOG_DIR)\n",
    "        train_summary_dir = os.path.join(LOG_DIR, \"train\")\n",
    "        test_summary_dir = os.path.join(LOG_DIR, \"test\")\n",
    "        if not os.path.exists(train_summary_dir):\n",
    "            os.makedirs(train_summary_dir)\n",
    "        if not os.path.exists(test_summary_dir):\n",
    "            os.makedirs(test_summary_dir)\n",
    "        self.train_writer = tf.summary.FileWriter(train_summary_dir, self.session.graph)\n",
    "        self.test_writer = tf.summary.FileWriter(test_summary_dir)\n",
    "    \n",
    "        # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # Add ops to save and restore all the variables.\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Error statistics\n",
    "        self.err_per_class = np.zeros([43])\n",
    "\n",
    "    def weight_variable(self,shape,stddev=0.1):\n",
    "        initial = tf.truncated_normal(shape,stddev=stddev)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.1, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(self, x, W, b, strides=1):\n",
    "        # Conv2D wrapper, writh bias and relu activation\n",
    "        x = tf.nn.conv2d(x, W, strides=[1,strides,strides,1], padding='VALID')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        #return tf.nn.relu(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return tf.nn.dropout(x, self.keep_prob)\n",
    "    \n",
    "    def create_tsc_net(self):   \n",
    "        \"\"\"\n",
    "        Create the tensorflow based layers, cost, optimizer, etc. \n",
    "        \"\"\"\n",
    "        # Hyperparameters\n",
    "        mu = 0\n",
    "        sigma = 0.1\n",
    "\n",
    "        # Dropout probability\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        with tf.name_scope(\"input_layer\"):\n",
    "            # Input layer: [batch_size, 32, 32, 3] - TODO: Tensorflow doesn't support tf.float64 yet.\n",
    "            self.img_in = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            tf.summary.histogram(\"input_img\", self.img_in)\n",
    "            \n",
    "        with tf.name_scope(\"layer1_conv\"):\n",
    "            # Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "            conv1_W = self.weight_variable(shape=[5, 5, 3, 12], stddev=sigma)\n",
    "            conv1_b = self.bias_variable(shape=[12])\n",
    "            self.conv1   = self.conv2d(self.img_in, conv1_W, conv1_b)\n",
    "            tf.summary.histogram(\"conv1_W\", conv1_W)\n",
    "            tf.summary.histogram(\"conv1_b\", conv1_b)\n",
    "            tf.summary.histogram(\"conv1\", self.conv1)\n",
    "            # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "            self.conv1 = tf.nn.max_pool(self.conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        with tf.name_scope(\"layer2_conv\"):\n",
    "            # Layer 2: Convolutional. Output = 10x10x16.\n",
    "            conv2_W = self.weight_variable(shape=[5, 5, 12, 32], stddev = sigma)\n",
    "            conv2_b = self.bias_variable(shape=[32])\n",
    "            self.conv2   = self.conv2d(self.conv1, conv2_W, conv2_b)\n",
    "            tf.summary.histogram(\"conv2_W\", conv2_W)\n",
    "            tf.summary.histogram(\"conv2_b\", conv2_b)\n",
    "            tf.summary.histogram(\"conv2\", self.conv2)\n",
    "            # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "            self.conv2 = tf.nn.max_pool(self.conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # Flatten. Input = 5x5x16. Output = 400.\n",
    "        fc0 = flatten(self.conv2)\n",
    "\n",
    "        with tf.name_scope(\"layer3_fc\"):\n",
    "            # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "            fc1_W = self.weight_variable(shape=[800, 200], stddev = sigma)\n",
    "            fc1_b = self.bias_variable(shape=[200])\n",
    "            fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "            # Activation.\n",
    "            fc1    = tf.nn.relu(fc1)\n",
    "            # Dropout\n",
    "            fc1_dropout = tf.nn.dropout(fc1, self.keep_prob)\n",
    "            tf.summary.histogram(\"fc1_W\", fc1_W)\n",
    "            tf.summary.histogram(\"fc1_b\", fc1_b)\n",
    "            tf.summary.histogram(\"fc1\", fc1)\n",
    "            tf.summary.histogram(\"fc1_dropout\", fc1_dropout)\n",
    "            \n",
    "        with tf.name_scope(\"layer4_fc\"):\n",
    "            # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "            fc2_W  = self.weight_variable(shape=[200, 128], stddev = sigma)\n",
    "            fc2_b  = self.bias_variable(shape=[128])\n",
    "            fc2    = tf.matmul(fc1_dropout, fc2_W) + fc2_b\n",
    "            # Activation.\n",
    "            fc2    = tf.nn.relu(fc2)\n",
    "            # Dropout\n",
    "            fc2_dropout = tf.nn.dropout(fc2, self.keep_prob)\n",
    "            tf.summary.histogram(\"fc2_W\", fc2_W)\n",
    "            tf.summary.histogram(\"fc2_b\", fc2_b)\n",
    "            tf.summary.histogram(\"fc2\", fc2)\n",
    "            tf.summary.histogram(\"fc2_dropout\", fc2_dropout)\n",
    "            \n",
    "        with tf.name_scope(\"layer5_fc\"):\n",
    "            # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "            fc3_W  = self.weight_variable(shape=[128, 43], stddev = sigma)\n",
    "            fc3_b  = self.bias_variable(shape=[43])\n",
    "            self.logits = tf.matmul(fc2_dropout, fc3_W) + fc3_b\n",
    "            tf.summary.histogram(\"fc3_W\", fc3_W)\n",
    "            tf.summary.histogram(\"fc3_b\", fc3_b)\n",
    "            tf.summary.histogram(\"logits\", self.logits)\n",
    "            \n",
    "    def create_tf(self):\n",
    "        \"\"\"\n",
    "        Loss/accuracy function and optimizer definition.\n",
    "        \"\"\"\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "        self.label_truth = tf.placeholder(tf.float32, [None,43])\n",
    "        self.loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits, self.label_truth))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        #self.prediction = tf.nn.softmax(self.logits)        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.logits, 1), \n",
    "                                    tf.argmax(self.label_truth, 1)), tf.float32))\n",
    "        \n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        \n",
    "    def train(self,X,y,lr,i):\n",
    "        summary, _, loss, accuracy = self.session.run(\n",
    "                [self.merged_summaries, self.optimizer, self.loss, self.accuracy],\n",
    "                feed_dict={\n",
    "                    self.img_in: X.astype(np.float32),\n",
    "                    self.label_truth: y.astype(np.float32),\n",
    "                    self.keep_prob: TRAIN_DROPOUT,\n",
    "                    self.learning_rate: lr\n",
    "                })\n",
    "        # Record summary every N batches\n",
    "        if i%50 == 0:\n",
    "            print('training: step {0:5d}, lr {1:8.7f}, accuracy {2:8.2f}%, loss {3:8.2f}'.format(i, lr, accuracy*100, loss))\n",
    "            self.train_writer.add_summary(summary, i)\n",
    "\n",
    "    def val(self,X,y,i,summary_on):\n",
    "        summary, loss, accuracy = self.session.run(\n",
    "                [self.merged_summaries, self.loss, self.accuracy],\n",
    "                feed_dict={\n",
    "                    self.img_in: X.astype(np.float32),\n",
    "                    self.label_truth: y.astype(np.float32),\n",
    "                    self.keep_prob: TEST_DROPOUT\n",
    "                })\n",
    "        print('validation: step {0:5d}, accuracy {1:8.2f}%, loss {2:8.2f}'.format(i, accuracy*100, loss))\n",
    "        if summary_on:\n",
    "            self.test_writer.add_summary(summary, i)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        logits = self.session.run(\n",
    "                [self.logits],\n",
    "                feed_dict={\n",
    "                    self.img_in: X.astype(np.float32),\n",
    "                    self.keep_prob: TEST_DROPOUT\n",
    "                })\n",
    "        # Predict class catogery\n",
    "        for i in range(len(X)):\n",
    "            print(\"Prediction: img {}, class {}\".format(i, np.argmax(logits[0][i])))\n",
    "        return logits\n",
    "\n",
    "    def err_statistics(self,X,y):\n",
    "        logits = self.session.run(\n",
    "                [self.logits],\n",
    "                feed_dict={\n",
    "                    self.img_in: X.astype(np.float32),\n",
    "                    self.label_truth: y.astype(np.float32),\n",
    "                    self.keep_prob: TEST_DROPOUT\n",
    "                })\n",
    "        # Error counts per class/category\n",
    "        for i in range(len(y)):\n",
    "            if np.argmax(logits[0][i]) != np.argmax(y[i]):\n",
    "                self.err_per_class[np.argmax(y[i])] += 1\n",
    "        print('err statistics: err_per_class = {}'.format(self.err_per_class))\n",
    "        return self.err_per_class\n",
    "\n",
    "    def saveParam(self):\n",
    "        if not os.path.exists(MODEL_DIR):\n",
    "            os.makedirs(MODEL_DIR)\n",
    "        checkpoint_path = os.path.join(MODEL_DIR, \"model.ckpt\")\n",
    "        filename = self.saver.save(self.session, checkpoint_path)\n",
    "        print(\"Model saved in file: %s\" % filename)\n",
    "\n",
    "    def restoreParam(self):\n",
    "        if not os.path.exists(MODEL_DIR):\n",
    "            sys.exit(\"No such dir to restore parameters! Exiting.\")\n",
    "        checkpoint_path = os.path.join(MODEL_DIR, \"model.ckpt\")\n",
    "        self.saver.restore(self.session, checkpoint_path)\n",
    "        print(\"Model restored from file: %s\" % checkpoint_path)\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # The following two functions are specifically added for TDB\n",
    "    #--------------------------------------------------------------\n",
    "    def tdb_probe(self):\n",
    "        \"\"\" \n",
    "        Probe signals for TDB\n",
    "        \"\"\"\n",
    "        # weight variables are of type tf.Variable, so we need to find the corresponding tf.Tensor instead\n",
    "        self.g=tf.get_default_graph()\n",
    "        #self.p1=tdb.plot_op(viz.viz_conv_weights,inputs=[self.g.as_graph_element(self.conv1_W)])\n",
    "        self.p1=tdb.plot_op(viz.viz_conv_out,inputs=[self.g.as_graph_element(self.conv1)])\n",
    "        self.p2=tdb.plot_op(viz.viz_conv_out,inputs=[self.g.as_graph_element(self.conv2)])\n",
    "        #self.p3=tdb.plot_op(viz.viz_conv_hist,inputs=[self.g.as_graph_element(self.conv1_W)])\n",
    "\n",
    "    def tdb_process(self,X):\n",
    "        local_feed_dict = {\n",
    "                    self.img_in: X.astype(np.float32),\n",
    "                    self.keep_prob: TEST_DROPOUT\n",
    "                }\n",
    "        logits = self.session.run(\n",
    "                [self.logits],\n",
    "                feed_dict=local_feed_dict\n",
    "            )\n",
    "        # run node and visualization node\n",
    "        status,result=tdb.debug([self.p1,self.p2], feed_dict=local_feed_dict, breakpoints=None, break_immediately=False, session=self.session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing images with trained net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from file: ./model/2xLeNet_256_8e-4_dropout/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "TEST_DROPOUT=1.0\n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    img_max = np.max(image_data)\n",
    "    img_min = np.min(image_data)\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "\n",
    "    img_normed = a + (b-a)*(image_data - img_min)/(img_max - img_min)\n",
    "    #print(np.max(img_normed))\n",
    "    #print(np.min(img_normed))\n",
    "    return img_normed\n",
    "\n",
    "def normalize_color(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data on per channel basis. \n",
    "    \"\"\"\n",
    "    img_normed_color = np.zeros_like(image_data, dtype=float)\n",
    "    for ch in range(image_data.shape[3]):\n",
    "        tmp = normalize_grayscale(image_data[:,:,:,ch])\n",
    "        img_normed_color[:,:,:,ch] = tmp\n",
    "    #print(np.max(img_normed_color))\n",
    "    #print(np.min(img_normed_color))\n",
    "    return img_normed_color\n",
    "\n",
    "# Create TSC NN and restore trained parameters\n",
    "mynet=tsc_net()\n",
    "# TDB probe signals\n",
    "mynet.tdb_probe()\n",
    "\n",
    "# Restore trained parameters\n",
    "mynet.restoreParam()\n",
    "\n",
    "# Load internet images and make prediction\n",
    "img_in = np.zeros([10,32,32,3])\n",
    "for i in range(10):\n",
    "    img=cv2.imread('./internet_traffic_signs/'+str(i)+'.png')\n",
    "    img = cv2.resize(img,(32,32))\n",
    "    img = img.reshape(1,32,32,3)\n",
    "    # Normalize the image between [0.1, 0.9]\n",
    "    img_in[i] = normalize_color(img)\n",
    "\n",
    "\n",
    "# Predict\n",
    "mynet.tdb_process(img_in[1:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Try another image\n",
    "\n",
    "It is time-consuming for step 3, but once model is loaded with restored parameters, step 4 here becomes very fast. \n",
    "\n",
    "This is \"online\", which is an advantage comparing to image summary of Tensorboard. \n",
    "\n",
    "<img src=\"tdb_tsc_net.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mynet.tdb_process(img_in[8:9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
